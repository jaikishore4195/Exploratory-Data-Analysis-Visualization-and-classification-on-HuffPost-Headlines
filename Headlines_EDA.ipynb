{
  "cells": [
    {
      "metadata": {
        "_uuid": "c1dc2c02c7d9d68d35479d4913a1cd06afbd9e3f"
      },
      "cell_type": "markdown",
      "source": "### importing libraries"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e677d20233551220ef18c85e1f71eaf680265d4"
      },
      "cell_type": "code",
      "source": "import json\nimport re\nfrom stop_words import get_stop_words\nimport tensorflow as tf\nimport keras\nfrom collections import Counter",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc14ccd8c7ec14089e4c82f4f6a4a80014a101ed"
      },
      "cell_type": "code",
      "source": "import nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import TruncatedSVD",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1fa7ee63a6a9089e399eab31c34abd4f93d5d6c7"
      },
      "cell_type": "code",
      "source": "import os\nos.getcwd()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b240934797daca1b2c3024c3d9d9269603257ea9"
      },
      "cell_type": "markdown",
      "source": "### reading in the json data into a list"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "54b5b72d7d4c84a2aed1d13c7c00390acbf0fdd5"
      },
      "cell_type": "code",
      "source": "data = []\nfor line in open(\"../input/News_Category_Dataset.json\",'r'):\n    data.append(json.loads(line))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c54a8b8d33c965acbdf9815cadd140d1f5f24ce1"
      },
      "cell_type": "markdown",
      "source": "### loading the common english stop words into a list"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "273140416ef54d9d9187303e2075c17c9f035b0e"
      },
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nstop_words = list(get_stop_words('en'))\nstop_words = stop_words + ['']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fa8a6a457c3196f3ab7705946b3c4250c9196a1f"
      },
      "cell_type": "markdown",
      "source": "## fucntion description\n##### 1) Function uses the JSON data imported into the list data\n##### 2) using the list data we extract headline and short description and store it it seperate lists\n##### 3) extracted the category and stored it in a different list\n##### 4) Then did a stop word removal on the data so that the n-gram frequency is not dominated by stop words\n##### 5) Converted the file list to a NLTK Text so that we can access rich collection of NLTK functions\n##### 6) Used NLTK Freqdist and bi-grams and Counter from Collections to get the unigram and bigram counts\n##### 7) Convereted the dictionary to a pandas file and then exported the file to disk using to_excel"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "c05a11a800453c8f8a0bedf911bc633320d0b55a"
      },
      "cell_type": "code",
      "source": "def ngram_counter():\n    sentences=[]\n    global data\n    target=[]\n    for i in range(0,len(data)):\n        if data[i]['headline'] != '':\n            data[i]['headline'] = data[i]['headline'].replace(\"'s\",\" is\")\n            data[i]['short_description'] = data[i]['short_description'].replace(\"'s\",\" is\")\n            data[i]['headline'] = re.sub(r'[^\\w\\s]','',data[i]['headline']).lower()\n            data[i]['short_description'] = re.sub(r'[^\\w\\s]','',data[i]['short_description']).lower()\n            sentences.append(data[i]['headline']+' '+data[i]['short_description'])\n            target.append(data[i]['category'])\n    data_new=[]\n    for sentence in sentences:\n        for word in sentence.split(\" \"):\n            if word not in stop_words:\n                data_new.append(word)\n    \n    nlp = nltk.Text(data_new)\n    \n    freq = nltk.FreqDist(nlp)\n    bi_grams = nltk.bigrams(nlp)\n    bi_grams = list(bi_grams)\n    bi_grams_counts = Counter(list(bi_grams))\n    \n    unigram = (pd.DataFrame.from_dict(data=freq,orient='index')).rename(columns = {0:'frequency'})\n    bi_gram = (pd.DataFrame.from_dict(data=bi_grams_counts,orient='index')).rename(columns = {0:'frequency'})\n    \n    writer = pd.ExcelWriter('ngrams4.xlsx')\n    \n    unigram.to_excel(writer,sheet_name='one_gram')\n    bi_gram.to_excel(writer,sheet_name='bi_gram')\n    writer.save()\n    \n    return nlp,bi_grams_counts,sentences,target",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8a8d86069630b30ea1c8b33cfaf824d05510f565"
      },
      "cell_type": "markdown",
      "source": "#### checking the function execution time\n\n##### 1) importing the time libary which returns the unix time at the instant\n##### 2) saving the start time to a variable start\n##### 3) running the ngrams function\n##### 4) running the time.time() to save the stop time\n##### 5) printing the difference between start and stop which is the time taken by the function ngram_counter\n\n### ngram_counter() in my testing ran with a mean value of 87 seconds."
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "b651fc33ec9a14130815adb8f07664408821e590"
      },
      "cell_type": "code",
      "source": "import time\nstart = time.time()\nnlp,bi_grams_counts,sentences,target = ngram_counter()\nstop = time.time()\nprint(stop-start)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c062d4e6acb6dadcc60b0ebe4c6581e04e0d4010"
      },
      "cell_type": "markdown",
      "source": "### displaying the most common unigrams"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "c14e129de0aea16627470f28b35cda370b734c62"
      },
      "cell_type": "code",
      "source": "freq = nltk.FreqDist(nlp)\nfreq.most_common(20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2beea2c60e254acd3b83b35c84a0e3a574689bc0"
      },
      "cell_type": "markdown",
      "source": "### displaying the most common bigrams"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "2c7c4ee1971794eb8b13af1863e1d7780d53d087"
      },
      "cell_type": "code",
      "source": "bi_grams_counts.most_common(20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eb816c4f7aa7b5cd01fd65483ff6557e6a0bf371"
      },
      "cell_type": "markdown",
      "source": "### checking the number of words in the corpus and also checking the lexical diversity of the corpus"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "be9a54c44d4874d3e65059ffefa2915f40436d15"
      },
      "cell_type": "code",
      "source": "#total of around 2 million words with lexical diversity of 0.03\nprint(\"number of words: \",len(nlp))\nprint(\"lexical diversity: \",len(set(nlp)) / len(nlp))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "01c6f9ffc030901da38b5e905d3aa4f3d05ba719"
      },
      "cell_type": "markdown",
      "source": "### checking for collocation"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "10e44843f4cc02fb9f1162806ebb4b1553114227"
      },
      "cell_type": "code",
      "source": "nlp.collocations()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "81c9fa8b82aac1efc9f44f992649aa0049ffb8f4"
      },
      "cell_type": "markdown",
      "source": "### plotting the most frequent unigrams"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "668c714a4e1d7edd4004e7267132c3479e71b9dd"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "d3e672060b8f20fedcb218567d31ebf02d827bcc"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(14,12))\nplt.bar([i[0] for i in freq.most_common(50)],[i[1] for i in freq.most_common(50)],align = 'center',alpha = 0.5,color = 'r')\nplt.xticks([i[0] for i in freq.most_common(50)],rotation='vertical')\nplt.ylabel('Frequency')\nplt.xlabel('Top 50 high frequency word tokens')\nplt.title('Top 50 words')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b8922ccc6a252d1269c9ccb54f6fd7cb2e34670"
      },
      "cell_type": "markdown",
      "source": "### Plotting the most frequent bigrams"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "3cfa7eb1bb18c530972af341d54e6bb268969df5"
      },
      "cell_type": "code",
      "source": "plt.figure(figsize=(14,12))\nplt.bar([i[0][0] + ' ' + i[0][1] for i in bi_grams_counts.most_common(50)],[i[1] for i in bi_grams_counts.most_common(50)],align = 'center',alpha = 0.5,color = 'r')\nplt.xticks([i[0][0] + ' ' + i[0][1] for i in bi_grams_counts.most_common(50)],rotation='vertical')\nplt.ylabel('Frequency')\nplt.xlabel('Top 50 high frequency word tokens')\nplt.title('Top 50 words')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "488c999ef35857acd491a9610f176b737d4cfcc8"
      },
      "cell_type": "markdown",
      "source": "### doing a word cloud on the corpus"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "12550f597e04404c8a2410a4009603abd6a2635e"
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nwordcloud = WordCloud(stopwords=stop_words,background_color='white', random_state=123).generate(\" \".join(sentence for sentence in sentences))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "af13a82c124a3d522ef4714735498fcb94b41b25"
      },
      "cell_type": "code",
      "source": "print(wordcloud)\nfig = plt.figure(figsize=(8,8))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "542589d0dbbcfe209f75472e66cbbe5c3c8cd5f2"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a65ef7d06783207e68b42b9fcb4954929b65db18"
      },
      "cell_type": "markdown",
      "source": "### Performing classification on the data so that we train a model such that given a sentence and its tag it is trained to classify the tag and on the test data we would be giving in the test sentence and the model will predict the tag\n\n### And i find that these trained classifiers on this data have practical application as this can be used to automatically sort the news article into a subcategory without analyzing the entire article there by saving a lot of compute."
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "fd8720c96f727ab8e88af49dceef712bd454ebda"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d147b0701603ef0f77dcfcfdfa085f4aae4a1a4d"
      },
      "cell_type": "markdown",
      "source": "### And as there are 31 classes so even though the output accuracy is low as 50% i feel that is a good result and not a random values being predicted by the model  as there are 31 classes. \n\n### in dichotomous classification an accuracy of 50 % is like model throwing random values out but in our case 50 % accuracy means model has learnt to discriminate 50 % of the data correctly to their respective class."
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "8c817169c18d74c154f3051e62dc3152a73d0968"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0df0d196cd1e4b3ff43a5d4d0168083e1ffc9d9a"
      },
      "cell_type": "markdown",
      "source": "### peroforming a train test split on the corpus"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "4bb9756c0c3364c732c7d16623bd34afd529011b"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "51f9b9fb31eb7c4cfd97e7063c051024975fae7e"
      },
      "cell_type": "code",
      "source": "x_train, x_test, y_train, y_test = train_test_split(sentences, target, test_size= 0.2, random_state=123)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d7159d51ceb968a1d52dd0844b8311a6dd8fd89a"
      },
      "cell_type": "markdown",
      "source": "### Creating the TFIDF matrix using Sklearn"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "db488b3973567a0511619e0d12e008088c908758"
      },
      "cell_type": "code",
      "source": "word_vectorizer = TfidfVectorizer(strip_accents='unicode',ngram_range=(1,1))\n\nword_vectorizer.fit(x_train)\nx_train_word_features = word_vectorizer.transform(x_train)\n\ntest_features = word_vectorizer.transform(x_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c28be5edd20aea4af43c50e53ba690379aa364ba"
      },
      "cell_type": "markdown",
      "source": "### as we would be calculating accuracy multiple times so writing an accuracy function to avoid repetation the code"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "d37a0f1d1300e61d59effbc9ae15c64a533c0442"
      },
      "cell_type": "code",
      "source": "def accuracy(model,train_pred):\n    train_accuracy = accuracy_score(y_pred=train_pred,y_true=y_train)\n    pred_test=model.predict(test_features)\n    test_accuracy = accuracy_score(y_pred=pred_test,y_true=y_test)\n    \n    return train_accuracy,test_accuracy",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1b7811e4ca5756d6c5299cfc9577f6545459aeed"
      },
      "cell_type": "markdown",
      "source": "### as the vocabulary is very high so the tfidf matrix is very high dimensional and sparse so performing matrix decompostion and choosing first 150 columns"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "fff2c35bc3c8592c81e389713351ca4d25a72320"
      },
      "cell_type": "code",
      "source": "svd = TruncatedSVD(n_components=150)\nx_train_word_features = svd.fit_transform(x_train_word_features)\ntest_features = svd.transform(test_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "792e5041a951d7d18d77f71f144727d5ea4e1608"
      },
      "cell_type": "markdown",
      "source": "### performing logistic regression on the data and checking the train and test accuracy"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "351469365936a6c512b2762d8a51e30a4bd5c952"
      },
      "cell_type": "code",
      "source": "model_logistic=LogisticRegression()\nmodel_logistic.fit(x_train_word_features,y_train)\npred=model_logistic.predict(x_train_word_features)\n(tr_accu,tes_accu) = accuracy(model_logistic,pred)\nprint(\"train accuracy: \",tr_accu,\" test accuracy: \",tes_accu)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "834e32ecaa099a78354a66048f4d2d647c7581e2"
      },
      "cell_type": "markdown",
      "source": "### building a Multi layer perceptron to predict the output using keras"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "176bd63e361e9c4a362e04591a5c41a225f35e27"
      },
      "cell_type": "code",
      "source": "y_train = pd.get_dummies(y_train)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "f2dcb143c73b248cecbc5f1d80715ccaa95d84cd"
      },
      "cell_type": "code",
      "source": "inp = keras.layers.Input(shape=[150])\nlayer_1 = keras.layers.Dense(300,activation='relu')(inp)\ndrop1 = keras.layers.Dropout(0.5)(layer_1)\nlayer_2 = keras.layers.Dense(100,activation='relu')(drop1)\ndrop2 = keras.layers.Dropout(0.5)(layer_2)\nout = keras.layers.Dense(31,activation='softmax')(drop2)\nmodel_dnn = keras.models.Model(inputs = inp,outputs=out)\nmodel_dnn.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel_dnn.fit(x_train_word_features,y_train,batch_size=32,epochs=4,shuffle=True,validation_split=0.1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "d07f847664c3599b3f96dd1e403dabb139bc8734"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}